# Preparar e pr√©-processar um dataset pequeno para experimenta√ß√£o
> A tarefa consiste em pegar um conjunto de dados pequeno e deix√°-lo "arrumado" para que o computador consiga aprender com ele.

O pr√©- processamento consistiria em:

- **Organizar os dados** ‚Üí tirar duplicados, completar onde falta informa√ß√£o ou decidir jogar fora dados incompletos.


- **Traduzir informa√ß√µes em n√∫meros** ‚Üí o computador entende melhor n√∫meros do que palavras.
    - Ex: transformar ‚ÄúMasculino/Feminino‚Äù em `0` e `1`.
- **Colocar tudo na mesma escala** ‚Üí se uma coluna tem valores como ‚Äúidade = 25‚Äù e outra ‚Äúsal√°rio = 8000‚Äù, o modelo pode dar mais import√¢ncia pro sal√°rio s√≥ porque √© um n√∫mero maior. A normaliza√ß√£o serve pra ‚Äúequilibrar‚Äù isso.
- **Separar treino e teste** ‚Üí √© como estudar para uma prova: voc√™ aprende com um peda√ßo do material (treino) e depois se testa com outro peda√ßo (teste) para ver se realmente aprendeu.

Ent√£o, pr√©-processar um dataset pequeno serve para limpar, organizar e transformar os dados em um formato que o modelo de intelig√™ncia artificial consiga entender e aprender de verdade.
Assim, quando voc√™ for treinar o modelo (tipo uma √°rvore de decis√£o, uma regress√£o ou uma rede simples), ele n√£o vai se confundir com valores faltando, categorias em texto ou n√∫meros em escalas muito diferentes.
### Pr√©- requisitos para essa tarefa:
- [x]  Python instalado

```jsx
python --version

```

- [x]  Bibliotecas :
- `numpy` (c√°lculos num√©ricos)
- `pandas` (manipula√ß√£o de dados)
- `matplotlib` / `seaborn` (visualiza√ß√£o)
- `scikit-learn` (modelos de machine learning)

```jsx
python -m pip show scikit-learn
//caso nao apare√ßa baixe usando 
conda install scikit-learn
//ou 
pip install scikit-learn

```

- [x]  Editor para rodar os C√≥digos ( Jupiter Notebook)
- [x]  Conhecimentos b√°sicos de Python (m√≠nimo necess√°rio)
### Realizando a tarefa:
#### üö¶ Passo a passo:
1. Carregar o dataset (ex.: Iris, CSV etc.).
```
iris = load_iris()
df = pd.DataFrame(iris.data, columns=iris.feature_names)
df['target'] = iris.target
```
2. Inspecionar os dados (ver tamanho, tipos, valores faltantes, classes).
```
# tamanho (linhas, colunas)
print("Shape:", df.shape)

# primeiras linhas
print(df.head())

# tipos de dados
print(df.info())

# valores faltantes
print("Valores faltantes por coluna:\n", df.isnull().sum())

# distribui√ß√£o do target
print("Classes dispon√≠veis:", df['target'].unique())
print("Contagem por classe:\n", df['target'].value_counts())
```
3. Limpar/transformar os dados (tratar valores nulos, remover colunas in√∫teis, ajustar formatos).
```
# exemplo: remover colunas in√∫teis (n√£o necess√°rio no Iris)
# df = df.drop(columns=['coluna_irrelevante'])

# exemplo: tratar valores nulos
df = df.fillna(df.mean(numeric_only=True))  # preenche nulos com m√©dia
```
4. Dividir em features (X) e target (y) (o que entra no modelo e o que queremos prever).
```
# X = dados de entrada (features)
X = df.drop('target', axis=1)

# y = o que queremos prever (r√≥tulo / classe)
y = df['target']

print("Features (X):")
print(X.head())
print("\nTarget (y):")
print(y.head())
```
5. Separar treino e teste (train_test_split) ‚Äî muito importante para garantir que o modelo seja avaliado de forma justa.
```
from sklearn.model_selection import train_test_split

X_train, X_test, y_train, y_test = train_test_split(
    X, y, test_size=0.2, random_state=42, stratify=y
)

print("Tamanho treino:", X_train.shape)
print("Tamanho teste:", X_test.shape)
```
6. Aplicar transforma√ß√µes:
- 6.1 Imputa√ß√£o de valores faltantes
```
from sklearn.impute import SimpleImputer

imputer = SimpleImputer(strategy='mean')
X_train_imputed = imputer.fit_transform(X_train)
X_test_imputed = imputer.transform(X_test)

```
- 6.2 Normaliza√ß√£o / Padroniza√ß√£o
```
from sklearn.preprocessing import StandardScaler

scaler = StandardScaler()
X_train_scaled = scaler.fit_transform(X_train_imputed)
X_test_scaled = scaler.transform(X_test_imputed)
```
- 6.3 Codifica√ß√£o de vari√°veis categ√≥ricas (se existissem)
```
from sklearn.preprocessing import OneHotEncoder

encoder = OneHotEncoder(sparse_output=False, handle_unknown='ignore')
categorical_data = [['vermelho'], ['azul'], ['verde']]
encoded = encoder.fit_transform(categorical_data)

print(encoded)  # vira n√∫meros bin√°rios (one-hot)

```

#### ‚úÖ Forma 1 ‚Äî Usando um dataset nativo do Scikit-learn

O Scikit-learn j√° traz datasets pequenos para pr√°tica (Iris, Breast Cancer, Digits, Wine...).

No arquivo   tem a demostra√ß√£o da prepara√ß√£o e pr√©-processamento do Dataset Iris `(conjunto de dados que  consiste em 50 amostras de cada uma das tr√™s esp√©cies de Iris ( Iris setosa, Iris virginica e Iris versicolor)`.
<img width="1898" height="823" alt="image" src="https://github.com/user-attachments/assets/a0407a65-35bf-4bdf-a7a7-0e9a508dd772" />


#### ‚úÖ Forma 2 ‚Äî Usando um dataset de CSV (ex.: baixado da internet)
> ##### üîπ  Onde conseguir datasets pequenos
> Aqui alguns lugares √≥timos:
> 1. **Nativos no Scikit-learn** ‚Üí Iris, Wine, Breast Cancer, Digits.
> 2. **Seaborn** ‚Üí vem com v√°rios datasets prontos (`sns.load_dataset("tips")`).
> 3. **Kaggle** ‚Üí plataforma com milhares de datasets (precisa conta gratuita). `https://www.kaggle.com/datasets`
> 4. **UCI Machine Learning Repository** ‚Üí datasets cl√°ssicos. ` https://archive.ics.uci.edu/ml/index.php`
> 5. **GitHub** ‚Üí v√°rios reposit√≥rios com datasets em CSV.

#### ‚úÖ Forma 3 ‚Äî Criando um dataset aleat√≥rio (√∫til para teste r√°pido)
Voc√™ pode gerar dados fict√≠cios para treinar um modelo.

---
&nbsp;<br>
&nbsp;<br>
&nbsp;<br>
&nbsp;<br> 

